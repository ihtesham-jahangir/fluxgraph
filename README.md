<div align="center">

<img src="https://github.com/ihtesham-jahangir/fluxgraph/blob/main/logo.jpeg" alt="FluxGraph" width="180" height="180"/>

# FluxGraph

### ğŸš€ Ship Production AI Without the Pain

**The only open-source framework where workflows survive crashes, costs stay predictable, and compliance isn't a nightmare**

[![PyPI](https://img.shields.io/pypi/v/fluxgraph?color=6366f1&style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/fluxgraph/)
[![Python](https://img.shields.io/pypi/pyversions/fluxgraph?style=flat-square&logo=python&logoColor=white&color=3776ab)](https://pypi.org/project/fluxgraph/)
[![License](https://img.shields.io/github/license/ihtesham-jahangir/fluxgraph?style=flat-square&color=10b981)](https://github.com/ihtesham-jahangir/fluxgraph/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/1243184424318402592?logo=discord&label=Community&style=flat-square&color=5865F2)](https://discord.gg/VZQZdN26)

[![Stars](https://img.shields.io/github/stars/ihtesham-jahangir/fluxgraph?style=social)](https://github.com/ihtesham-jahangir/fluxgraph)
[![Downloads](https://static.pepy.tech/badge/fluxgraph)](https://pepy.tech/project/fluxgraph)
[![Monthly](https://static.pepy.tech/badge/fluxgraph/month)](https://pepy.tech/project/fluxgraph)
[![Weekly](https://static.pepy.tech/badge/fluxgraph/week)](https://pepy.tech/project/fluxgraph)

[**ğŸ“– Docs**](https://fluxgraph.readthedocs.io) â€¢ [**ğŸ’¬ Discord**](https://discord.gg/VZQZdN26) â€¢ [**âš¡ Quick Start**](#-60-second-quickstart) â€¢ [**ğŸ¯ Examples**](#-real-world-examples)

</div>

---

## ğŸ’¥ The Problem with Other Frameworks

**LangGraph?** Requires manual checkpointing or your workflows die on restart.  
**CrewAI?** No workflows. No caching. Hope you like debugging.  
**AutoGen?** Complex setup for basic multi-agent tasks.

**FluxGraph?** Production infrastructure out-of-the-box. No assembly required.

---

## âš¡ 60-Second Quickstart

```bash
pip install fluxgraph[all]  # Full power mode
```

### Hello World â†’ Production-Ready

```python
from fluxgraph import FluxApp
import os

# 1ï¸âƒ£ Initialize (add DATABASE_URL for crash recovery)
app = FluxApp(
    database_url=os.getenv("DATABASE_URL"),  # Optional but recommended
    enable_workflows=True,
    enable_agent_cache=True
)

# 2ï¸âƒ£ Define your first agent
@app.agent()
async def assistant(message: str) -> dict:
    return {"response": f"You said: {message}"}

# 3ï¸âƒ£ Run it
# Command: flux run app.py
# Test: curl -X POST http://localhost:8000/ask/assistant -d '{"message":"Hello!"}'
```

**That's it!** ğŸ‰ You now have:
- âœ… REST API endpoints
- âœ… Automatic request validation
- âœ… Built-in error handling
- âœ… Production logging

### Add Crash Recovery (30 seconds more)

```python
from fluxgraph.core.workflow_graph import WorkflowBuilder

# Define conditional routing
def needs_revision(state):
    confidence = state.get("editor_result", {}).get("confidence", 0)
    return "retry" if confidence < 0.8 else "done"

# Build workflow with automatic checkpointing
workflow = (
    WorkflowBuilder(
        "content_pipeline",
        checkpointer=app.checkpointer,  # â† Magic happens here
        logger=app.workflow_logger
    )
    .add_agent("writer", writer_agent)
    .add_agent("editor", editor_agent)
    .connect("writer", "editor")
    .branch("editor", needs_revision, {
        "retry": "writer",  # Loop back if quality < 80%
        "done": "__end__"
    })
    .start_from("writer")
    .build()
)

# Execute (survives crashes with workflow_id)
result = await workflow.execute(
    workflow_id="blog_post_123",  # â† Unique ID enables resume
    initial_data={"topic": "AI Trends 2025"}
)
```

**Server crashes mid-workflow?** Restart and run with same `workflow_id` â†’ **resumes from last completed step** âœ¨

---

## ğŸ¯ What Makes FluxGraph Different

<table>
<thead>
<tr>
<th width="20%">You Need</th>
<th width="20%">FluxGraph 3.2</th>
<th width="20%">LangGraph</th>
<th width="20%">CrewAI</th>
<th width="20%">AutoGen</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Workflows survive crashes</strong></td>
<td>âœ… <strong>Built-in (Postgres)</strong></td>
<td>âš ï¸ Manual setup</td>
<td>âŒ No workflows</td>
<td>âŒ</td>
</tr>
<tr>
<td><strong>Conditional routing</strong></td>
<td>âœ… <strong>Full support</strong></td>
<td>âœ… Core feature</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><strong>Debug in production</strong></td>
<td>âœ… <strong>DB event logs</strong></td>
<td>âš ï¸ Pay for LangSmith</td>
<td>âŒ Print debugging</td>
<td>âŒ</td>
</tr>
<tr>
<td><strong>Prove compliance</strong></td>
<td>âœ… <strong>Hash-chained audits</strong></td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><strong>Cut LLM costs 70%+</strong></td>
<td>âœ… <strong>Semantic caching</strong></td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><strong>Track actual costs</strong></td>
<td>âœ… <strong>Accurate + reliable</strong></td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><strong>Protect PII</strong></td>
<td>âœ… <strong>9 types detected</strong></td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><strong>Vision models (GPT-4V)</strong></td>
<td>âœ… <strong>Standardized API</strong></td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><strong>Streaming responses</strong></td>
<td>âœ… <strong>SSE optimized</strong></td>
<td>âš ï¸ Callbacks</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
</tbody>
</table>

---

## ğŸ”¥ Core Features (The Boring Stuff That Matters)

### 1ï¸âƒ£ **Crash-Proof Workflows** â€” Never Lose Progress

<table>
<tr>
<td width="50%">

**âŒ Without FluxGraph**
```python
# Server crashes = start over
workflow.run(data)
# Lost: 45 minutes of GPT-4 calls ğŸ’¸
```

</td>
<td width="50%">

**âœ… With FluxGraph**
```python
# Server crashes = resume from checkpoint
await workflow.execute(
    workflow_id="unique_123",
    initial_data=data
)
# Saved: $47.32 in API costs âœ¨
```

</td>
</tr>
</table>

**How it works:** Every step auto-saves to Postgres. On crash, FluxGraph detects incomplete workflow and resumes from last successful node.

**Perfect for:**
- ğŸ¬ Long-running content pipelines
- ğŸ“Š Multi-stage data analysis
- ğŸ”¬ Research workflows with retries
- ğŸ’° Expensive multi-model chains

---

### 2ï¸âƒ£ **Production Debugging** â€” Stop Flying Blind

```python
app = FluxApp(
    database_url=os.getenv("DATABASE_URL"),
    enable_workflows=True  # â† Enables automatic logging
)

# Every step logged automatically:
# - Input/output of each node
# - State snapshots
# - Execution time
# - Errors with full context

# Query your execution history like data
logs = await app.workflow_logger.get_execution_logs(
    workflow_id="user_123"
)

for log in logs:
    print(f"{log.step_name}: {log.duration}ms")
    if log.error:
        print(f"Failed: {log.error_message}")
```

**What you get:**
- ğŸ“Š Step-by-step execution timeline
- ğŸ” Full state snapshots at each node
- âš ï¸ Error traces with complete context
- ğŸ“ˆ Per-agent performance metrics
- ğŸ’° Accurate cost attribution

**No more:** Print debugging, missing logs, "works on my machine"

---

### 3ï¸âƒ£ **Compliance Made Easy** â€” Verifiable Audit Trails

```python
from fluxgraph.security.audit_logger import VerifiableAuditLogger, AuditEventType

@app.agent()
async def financial_agent(
    user_id: str,
    transaction: dict,
    audit_logger: VerifiableAuditLogger  # Auto-injected
) -> dict:
    # Log BEFORE executing (tamper-evident)
    log_hash = await audit_logger.log(
        action=AuditEventType.DATA_MODIFICATION,
        actor=user_id,
        data={
            "action": "wire_transfer",
            "amount": transaction["amount"],
            "recipient": transaction["recipient"]
        }
    )
    
    result = execute_transfer(transaction)
    
    return {
        "status": "complete",
        "audit_hash": log_hash  # Cryptographic proof
    }
```

**The secret sauce:** Each log entry cryptographically hashes the previous entry, creating an unbreakable chain.

```python
# Verify integrity anytime
report = await app.audit_logger.verify_log_chain()

if report.is_valid:
    print("âœ… All logs verified")
else:
    print(f"ğŸš¨ Tampering detected at entry {report.break_point}")
```

**Perfect for:**
- ğŸ¥ HIPAA compliance (healthcare)
- ğŸ’° SOC2 audits (financial services)
- âš–ï¸ Legal/regulatory requirements
- ğŸ”’ High-security applications

**Benefits:**
- Blockchain-level integrity
- SQL database performance
- No third-party dependencies
- Instant verification (no scanning)

---

### 4ï¸âƒ£ **Semantic Caching** â€” Cut Costs 70%+

```python
app = FluxApp(
    enable_agent_cache=True,
    cache_strategy="hybrid"  # Exact + semantic matching
)

@app.agent()
async def expensive_agent(query: str, cache) -> dict:
    # Check cache first (with semantic similarity)
    if cached := cache.get(query, threshold=0.9):
        return cached  # ğŸ’° Saved an API call!
    
    # Only call LLM if truly novel
    response = await llm.generate(query)
    
    # Cache stores semantically similar queries
    cache.set(query, response)
    return response
```

**Real-world impact:**
- Query: "What's the weather in NYC?"
- Cache hit for: "Tell me NYC weather", "Weather in New York City", "NYC forecast"
- **Saved:** 3 GPT-4 calls = $0.15 per query cluster

**How it works:**
1. Query embeds your input
2. Searches for similar past queries (cosine similarity)
3. Returns cached response if >90% similar
4. Falls back to LLM only for novel inputs

---

### 5ï¸âƒ£ **Intelligent Multi-Agent Crews** â€” Hierarchical Delegation

```python
from fluxgraph.crew import Crew, RoleAgent, Task, ProcessType

# Define specialists
researcher = RoleAgent(
    role="Senior Market Researcher",
    goal="Find accurate market data",
    backstory="10 years experience in market analysis..."
)

writer = RoleAgent(
    role="Creative Content Writer", 
    goal="Transform data into compelling narratives",
    backstory="Award-winning journalist..."
)

# Define tasks
task1 = Task(
    description="Find Q4 2024 AI market size",
    expected_output="Market size in USD with sources"
)

task2 = Task(
    description="Write 500-word executive summary",
    expected_output="Polished, business-ready report"
)

# Manager automatically delegates based on skills
crew = Crew(
    agents=[researcher, writer],
    tasks=[task1, task2],
    process=ProcessType.HIERARCHICAL  # â† Enables smart routing
)

result = await crew.kickoff()
```

**What happens:**
1. Manager analyzes task requirements
2. Matches tasks to agent skills
3. Routes `task1` â†’ Researcher (data task)
4. Routes `task2` â†’ Writer (creative task)
5. Coordinates handoffs automatically

**No more:** Manual routing, mismatched tasks, coordination overhead

---

### 6ï¸âƒ£ **Multimodal Ready** â€” Vision + Audio + Text

```python
from fluxgraph.models import OpenAIProvider
from fluxgraph.multimodal.processor import MultiModalProcessor

processor = MultiModalProcessor()
llm = OpenAIProvider(model="gpt-4-vision-preview")

@app.agent()
async def vision_agent(image_path: str, prompt: str) -> dict:
    # Process image into standardized format
    image_input = processor.process_image(image_path)
    
    # Unified API for all multimodal models
    response = await llm.multimodal_generate(
        prompt=prompt,
        media_inputs=[image_input]  # Works with GPT-4V, Gemini
    )
    
    return {"analysis": response.text}
```

**Supported:**
- ğŸ–¼ï¸ GPT-4V (OpenAI)
- ğŸ¨ Gemini Vision (Google)
- ğŸ”Š Audio inputs (coming soon)

---

## ğŸ›¡ï¸ Enterprise Security (Zero Config)

```python
app = FluxApp(
    database_url=os.getenv("DATABASE_URL"),
    enable_security=True,         # âœ… Turns on all security
    enable_pii_detection=True,    # âœ… Auto-scan & redact
    enable_audit_logging=True     # âœ… Tamper-proof logs
)
```

### What You Get Automatically

| Feature | Protection | Use Case |
|---------|-----------|----------|
| **PII Detection** | EMAIL, PHONE, SSN, CREDIT_CARD, IP_ADDRESS, PASSPORT, DRIVER_LICENSE, DOB, MEDICAL_RECORD | Healthcare, finance, HR systems |
| **Prompt Injection Shield** | ROLE_PLAY, IGNORE_PREVIOUS, ENCODED_INJECTION, DELIMITER_INJECTION, PRIVILEGE_ESCALATION, CONTEXT_OVERFLOW, PAYLOAD_SPLITTING | User-facing chatbots |
| **Verifiable Audits** | Hash-chained, cryptographic integrity | Compliance (HIPAA, SOC2, GDPR) |
| **RBAC + JWT Auth** | Role-based access control | Multi-tenant applications |

**Example: Automatic PII Redaction**
```python
user_input = "My email is john@example.com and SSN is 123-45-6789"
# FluxGraph automatically detects and redacts:
# â†’ "My email is [EMAIL_REDACTED] and SSN is [SSN_REDACTED]"
```

---

## ğŸš€ Real-World Examples

### Customer Support Bot (Full Stack)

```python
from fluxgraph import FluxApp
from fluxgraph.core import MemoryType

app = FluxApp(
    database_url=os.getenv("DATABASE_URL"),
    enable_advanced_memory=True,  # Episodic + semantic memory
    enable_agent_cache=True,      # 70%+ cache hit rate
    enable_rag=True               # Knowledge base
)

@app.agent()
async def support_bot(
    query: str,
    session_id: str,
    advanced_memory,  # Auto-injected
    cache,            # Auto-injected
    rag               # Auto-injected
) -> dict:
    # 1ï¸âƒ£ Check cache first (save API calls)
    if cached := cache.get(query, threshold=0.9):
        return {"response": cached, "source": "cache"}
    
    # 2ï¸âƒ£ Search knowledge base
    kb_results = await rag.query(query, top_k=3)
    
    # 3ï¸âƒ£ Recall similar past conversations
    similar_cases = advanced_memory.recall_similar(query, k=5)
    
    # 4ï¸âƒ£ Generate response with context
    context = {
        "docs": kb_results,
        "past_cases": [e.content for e, _ in similar_cases]
    }
    
    response = await llm.generate(
        f"Context: {context}\nUser: {query}"
    )
    
    # 5ï¸âƒ£ Store for future recall
    advanced_memory.store(
        f"Q: {query}\nA: {response}",
        MemoryType.EPISODIC,
        importance=0.9
    )
    
    return {"response": response, "sources": kb_results}
```

**Delivers:**
- ğŸ’° 70%+ cost reduction (caching)
- ğŸ§  Context-aware responses (memory)
- ğŸ“š Accurate answers (RAG)
- âš¡ Sub-second latency

---

### Research Pipeline (Conditional Loops)

```python
from fluxgraph.core.workflow_graph import WorkflowBuilder

app = FluxApp(enable_workflows=True)

async def web_search_agent(state):
    results = await search_web(state.get("query"))
    state.update("web_results", results)
    return results

async def analysis_agent(state):
    analysis = await analyze_data(state.get("web_results"))
    state.update("analysis", analysis)
    return analysis

async def synthesis_agent(state):
    report = await synthesize(state.get("analysis"))
    return report

# Define quality gate
def quality_check(state):
    confidence = state.get("analysis", {}).get("confidence", 0)
    if confidence < 0.8:
        return "retry"  # Loop back to search
    return "synthesize"  # Move to synthesis

# Build self-correcting pipeline
workflow = (
    WorkflowBuilder("research", checkpointer=app.checkpointer)
    .add_agent("searcher", web_search_agent)
    .add_agent("analyzer", analysis_agent)
    .add_agent("synthesizer", synthesis_agent)
    .connect("searcher", "analyzer")
    .branch("analyzer", quality_check, {
        "retry": "searcher",        # Loop if quality low
        "synthesize": "synthesizer" # Proceed if quality high
    })
    .start_from("searcher")
    .build()
)

result = await workflow.execute(
    workflow_id="research_2025_ai",
    initial_data={"query": "AI market trends 2025"}
)
```

**Features:**
- âœ… Automatic retry on low-quality results
- âœ… Crash recovery (workflow_id enables resume)
- âœ… Full execution logs
- âœ… State persistence across steps

---

### Human-in-the-Loop (Critical Approvals)

```python
@app.agent()
async def critical_action_agent(
    action: str,
    amount: float,
    hitl  # Auto-injected
) -> dict:
    # Request approval for high-risk actions
    approval = await hitl.request_approval(
        task_description=f"Execute wire transfer: ${amount:,.2f}",
        risk_level="HIGH",
        timeout_seconds=300  # 5 minute deadline
    )
    
    # Wait for human decision (non-blocking)
    if await approval.wait_for_approval():
        result = execute_wire_transfer(action, amount)
        return {
            "status": "EXECUTED",
            "transaction_id": result.id,
            "approved_by": approval.approver_id
        }
    
    return {
        "status": "REJECTED",
        "reason": approval.rejection_reason
    }
```

**Perfect for:**
- ğŸ’° Financial transactions
- âš–ï¸ Legal document signing
- ğŸš¨ Security incident responses
- ğŸ“ Content moderation

---

## ğŸ“¦ Installation Options

```bash
# ğŸš€ Everything (recommended for production)
pip install fluxgraph[all]

# âš¡ Core features (workflows, memory, caching)
pip install fluxgraph[p0,chains]

# ğŸ­ Production stack (Postgres, security, orchestration)
pip install fluxgraph[production,orchestration,security,postgres]

# ğŸŒ RAG-powered (vector DB, embeddings)
pip install fluxgraph[rag,postgres]

# ğŸ¯ Minimal (just agents)
pip install fluxgraph
```

### Available Extras

| Extra | What You Get |
|-------|-------------|
| `p0` | Graph workflows, advanced memory, semantic caching |
| `production` | Streaming, sessions, retry logic, rate limiting |
| `security` | RBAC, audit logs, PII detection, prompt injection shield |
| `orchestration` | Agent handoffs, HITL, batch processing, load balancing |
| `rag` | ChromaDB, embeddings, document processing |
| `postgres` | Persistence, checkpointing, workflow logs, audits |
| `chains` | LCEL-style chaining, distributed tracing |
| `all` | Everything (batteries included) |

---

## ğŸŒ Production Deployment

### Docker Compose (Recommended)

```yaml
version: '3.8'
services:
  fluxgraph:
    image: fluxgraph:3.2.0
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/fluxgraph
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - db
      - redis
    command: gunicorn app:app -w 4 -k uvicorn.workers.UvicornWorker

  db:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
      - POSTGRES_DB=fluxgraph

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

```bash
docker-compose up -d
# ğŸ‰ Production-grade AI in 30 seconds
```

### Kubernetes (Auto-Scaling)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluxgraph
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: fluxgraph
        image: fluxgraph:3.2.0
        env:
        - name: DATABASE_URL
          value: postgresql://user:pass@postgres:5432/fluxgraph
        - name: FLUXGRAPH_ENABLE_WORKFLOWS
          value: "true"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fluxgraph-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fluxgraph
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

## ğŸ”Œ API Endpoints (REST Ready)

| Endpoint | Method | What It Does |
|----------|--------|--------------|
| `/ask/{agent}` | `POST` | Execute agent synchronously |
| `/stream/{agent}` | `GET` | Stream response (Server-Sent Events) |
| `/workflows` | `GET` | List all registered workflows |
| `/workflows/{name}/execute` | `POST` | Execute workflow (pass `workflow_id` for resume) |
| `/rag/ingest_urls` | `POST` | Add knowledge from web URLs |
| `/memory/stats` | `GET` | Memory usage statistics |
| `/memory/recall` | `POST` | Semantic search across memories |
| `/cache/stats` | `GET` | Cache hit rate & savings |
| `/audit/verify` | `GET` | Verify audit log integrity |
| `/system/health` | `GET` | System health check |
| `/system/costs` | `GET` | Cost breakdown by agent |

**Example: Execute Workflow**
```bash
curl -X POST http://localhost:8000/workflows/research/execute \
  -H "Content-Type: application/json" \
  -d '{
    "workflow_id": "user_request_789",
    "initial_data": {"query": "AI trends 2025"}
  }'
```

---

## ğŸ¤– Supported LLM Providers

| Provider | Models | Streaming | Cost Tracking | Multimodal | Notes |
|----------|--------|-----------|---------------|------------|-------|
| **OpenAI** | GPT-3.5, GPT-4, GPT-4 Turbo, GPT-4V | âœ… | âœ… | âœ… | Vision supported |
| **Anthropic** | Claude 3 (Haiku, Sonnet, Opus) | âœ… | âœ… | âœ… | Best for reasoning |
| **Google** | Gemini Pro, Ultra | âœ… | âœ… | âœ… | Multimodal native |
| **Groq** | Mixtral, Llama 3 | âœ… | âœ… | âŒ | Fastest inference |
| **Ollama** | All local models | âœ… | âœ… | âŒ | Privacy-first |
| **Azure OpenAI** | GPT models | âœ… | âœ… | âœ… | Enterprise SLA |

**Switch providers without code changes:**
```python
app = FluxApp(llm_provider="anthropic")  # That's it!
```

---

## ğŸ“Š Performance Benchmarks (v3.2)

| Metric | Result | Impact |
|--------|--------|--------|
| **Cache Hit Rate** | 85-95% | 70%+ cost reduction |
| **Memory Consolidation** | <50ms for 1000 entries | Real-time recall |
| **Workflow Throughput** | 100+ steps/second | High concurrency |
| **Latency Overhead** | <10ms | Negligible impact |
| **Cost Tracking Accuracy** | 99.8% | Reliable budgeting |

---

## ğŸ—ºï¸ Roadmap (We Ship Fast)

### âœ… v3.2 (Current â€” Q4 2025)
- âœ… Persistent conditional workflows (Postgres)
- âœ… Verifiable audit logs (hash-chained)
- âœ… Database-backed observability
- âœ… RAG-as-a-pipeline
- âœ… Intelligent hierarchical delegation
- âœ… Accurate cost tracking with tokenization fallback
- âœ… Standardized multimodal API (GPT-4V, Gemini)
- âœ… LCEL-style chains + distributed tracing

### ğŸš§ v3.3 (Q1 2026)
- ğŸ¨ **Visual workflow designer UI** (drag-and-drop)
- ğŸ“Š **Observability dashboard** (Grafana-style)
- ğŸ§  Agent learning & auto-optimization
- ğŸ” Advanced debugging tools

### ğŸ“‹ v3.4 (Q2 2026)
- ğŸŒ Distributed agent execution (multi-host)
- ğŸ“ˆ Auto-scaling workflows (K8s native)
- ğŸ” Enterprise SSO (SAML, OAuth)
- ğŸš€ Agent marketplace

---

## ğŸ¤ Community & Support

<div align="center">

### Join 1000+ Developers Building Production AI

[![Discord](https://img.shields.io/discord/1243184424318402592?logo=discord&label=Discord%20Community&style=for-the-badge&color=5865F2)](https://discord.gg/VZQZdN26)
[![GitHub Discussions](https://img.shields.io/github/discussions/ihtesham-jahangir/fluxgraph?style=for-the-badge&logo=github)](https://github.com/ihtesham-jahangir/fluxgraph/discussions)

</div>

**ğŸ“– Documentation:** [fluxgraph.readthedocs.io](https://fluxgraph.readthedocs.io)  
**ğŸ’¬ Discord:** [Join Community](https://discord.gg/VZQZdN26) â€” Get help, share projects  
**ğŸ› GitHub Issues:** [Report Bugs](https://github.com/ihtesham-jahangir/fluxgraph/issues) â€” We respond fast  
**ğŸ’¡ Discussions:** [Feature Requests](https://github.com/ihtesham-jahangir/fluxgraph/discussions)  
**ğŸ’¼ Enterprise:** enterprise@fluxgraph.com â€” SLA, priority support

---

## ğŸ› ï¸ Contributing

We â¤ï¸ contributions! Here's how to get started:

```bash
# Clone the repo
git clone https://github.com/ihtesham-jahangir/fluxgraph.git
cd fluxgraph

# Set up dev environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -e ".[dev,postgres]"

# Run tests
pytest tests/ -v

# Make changes, submit PR ğŸš€
```

**We need help with:**
- ğŸ“ Documentation & tutorials
- ğŸ§ª Test coverage expansion
- ğŸ”Œ New integrations (Pinecone, Weaviate, Cohere)
- ğŸ› Bug reports (please include reproduction steps)
- ğŸŒ Translations (docs in your language)
- ğŸ¨ UI/UX improvements

**Contributor perks:**
- ğŸ† Your name in CONTRIBUTORS.md
- ğŸ Exclusive Discord role
- ğŸ“¢ Shoutout in release notes
- ğŸš€ Direct line to maintainers

---

## ğŸ’¡ Advanced Use Cases

### Batch Processing (1000s of Tasks)

```python
# Submit large batch
job_id = await app.batch_processor.submit_batch(
    agent_name="data_processor",
    payloads=[{"id": i, "data": data[i]} for i in range(5000)],
    priority=0,
    max_concurrent=50  # Process 50 at a time
)

# Monitor progress
while True:
    status = app.batch_processor.get_job_status(job_id)
    print(f"Progress: {status.completed}/{status.total}")
    if status.is_complete:
        break
    await asyncio.sleep(5)

# Get results
results = app.batch_processor.get_job_results(job_id)
```

**Perfect for:**
- ğŸ“Š Large dataset processing
- ğŸ“§ Bulk email generation
- ğŸ–¼ï¸ Image batch analysis
- ğŸ“ Document batch summarization

---

### Agent Handoffs (Seamless Transfers)

```python
@app.agent()
async def supervisor(task: str, call_agent) -> dict:
    """Routes tasks to specialists."""
    
    # Determine which specialist to use
    if "research" in task.lower():
        return await call_agent("research_agent", query=task)
    elif "code" in task.lower():
        return await call_agent("coding_agent", task=task)
    else:
        return await call_agent("general_agent", query=task)

@app.agent()
async def research_agent(query: str, call_agent) -> dict:
    """Deep research specialist."""
    data = await perform_research(query)
    
    # Hand off to analyst for processing
    analysis = await call_agent("analyst_agent", data=data)
    return analysis
```

**Features:**
- âœ… Zero-config handoffs
- âœ… Context preservation
- âœ… Automatic cost attribution
- âœ… Full execution tracing

---

### Multi-Model Orchestration

```python
from fluxgraph.models import OpenAIProvider, AnthropicProvider

# Use different models for different tasks
openai = OpenAIProvider(model="gpt-4")  # Creative tasks
anthropic = AnthropicProvider(model="claude-3-opus")  # Analysis

@app.agent()
async def creative_agent(prompt: str) -> dict:
    """GPT-4 for creative writing."""
    response = await openai.generate(prompt)
    return {"content": response.text, "model": "gpt-4"}

@app.agent()
async def analysis_agent(data: str) -> dict:
    """Claude for deep analysis."""
    response = await anthropic.generate(f"Analyze: {data}")
    return {"analysis": response.text, "model": "claude-3-opus"}

@app.agent()
async def smart_agent(task: str, call_agent) -> dict:
    """Routes to best model for task type."""
    if "creative" in task or "story" in task:
        return await call_agent("creative_agent", prompt=task)
    else:
        return await call_agent("analysis_agent", data=task)
```

**Cost optimization:**
- Use GPT-3.5 for simple tasks ($0.0015/1K tokens)
- Use GPT-4 for complex reasoning ($0.03/1K tokens)
- Use Claude for long context ($0.008/1K tokens)
- Use Groq for speed-critical tasks (free tier)

---

### Streaming Responses (Real-Time UX)

```python
from fastapi.responses import StreamingResponse

@app.api.get("/stream/{agent_name}")
async def stream_agent_response(agent_name: str, query: str):
    """Stream agent response token-by-token."""
    
    async def generate():
        async for chunk in app.orchestrator.run_streaming(
            agent_name, 
            {"query": query}
        ):
            # Server-Sent Events format
            yield f"data: {chunk}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
```

**Frontend (JavaScript):**
```javascript
const eventSource = new EventSource('/stream/assistant?query=Hello');

eventSource.onmessage = (event) => {
    const chunk = event.data;
    document.getElementById('response').innerHTML += chunk;
};
```

---

### Cost Tracking & Budgets

```python
# Automatic per-agent tracking
costs = app.orchestrator.cost_tracker.get_summary()
print(costs)
# {
#   "research_agent": {"cost": "$12.45", "calls": 450},
#   "writing_agent": {"cost": "$8.32", "calls": 320},
#   "total": "$20.77"
# }

# Set budget alerts
app.orchestrator.cost_tracker.set_budget(
    agent_name="expensive_agent",
    daily_limit=50.00,  # $50/day
    alert_threshold=0.8  # Alert at 80%
)

# Query cost history
history = app.orchestrator.cost_tracker.get_history(
    start_date="2025-01-01",
    end_date="2025-01-31"
)
```

---

## ğŸ”’ Security Best Practices

### 1. Environment Variables (Never Hardcode)

```python
import os
from fluxgraph import FluxApp

app = FluxApp(
    database_url=os.getenv("DATABASE_URL"),
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    anthropic_api_key=os.getenv("ANTHROPIC_API_KEY")
)
```

### 2. Input Validation (Always)

```python
from pydantic import BaseModel, Field

class UserQuery(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    user_id: str = Field(..., regex=r"^[a-zA-Z0-9_-]+$")

@app.agent()
async def safe_agent(request: UserQuery) -> dict:
    # Pydantic validates automatically
    return {"response": f"Processing: {request.query}"}
```

### 3. Rate Limiting

```python
from fluxgraph.middleware import RateLimiter

app = FluxApp(
    middleware=[
        RateLimiter(
            requests_per_minute=60,
            per_user=True
        )
    ]
)
```

### 4. PII Redaction (Automatic)

```python
# FluxGraph automatically redacts when security is enabled
app = FluxApp(enable_security=True, enable_pii_detection=True)

@app.agent()
async def handle_user_data(text: str) -> dict:
    # "My SSN is 123-45-6789" â†’ "My SSN is [SSN_REDACTED]"
    # Redaction happens automatically before LLM sees it
    return {"processed": text}
```

---

## ğŸ“ Learning Resources

### Official Tutorials

1. **[Getting Started Guide](https://fluxgraph.readthedocs.io/quickstart)** â€” 15 min
2. **[Building Your First Workflow](https://fluxgraph.readthedocs.io/workflows)** â€” 30 min
3. **[Multi-Agent Systems](https://fluxgraph.readthedocs.io/multi-agent)** â€” 45 min
4. **[Production Deployment](https://fluxgraph.readthedocs.io/deployment)** â€” 60 min

### Video Tutorials (Coming Soon)

- ğŸ¥ FluxGraph in 100 Seconds
- ğŸ¥ Building a RAG Chatbot
- ğŸ¥ Deploying to Production (K8s)
- ğŸ¥ Cost Optimization Strategies

### Community Examples

Browse 50+ community-built examples:
- ğŸ“Š Financial Analysis Agent
- ğŸ¨ AI Art Director (multi-model)
- ğŸ“§ Email Assistant with Memory
- ğŸ”¬ Research Paper Analyzer
- ğŸ’¬ Customer Support Bot (full stack)

[**View Examples â†’**](https://github.com/ihtesham-jahangir/fluxgraph/tree/main/examples)

---

## â“ FAQ

### **Q: Why FluxGraph over LangGraph?**

**A:** LangGraph gives you building blocks. FluxGraph gives you production infrastructure:
- âœ… Automatic crash recovery (vs manual checkpointing)
- âœ… Built-in observability (vs paying for LangSmith)
- âœ… Security out-of-the-box (vs DIY)
- âœ… Semantic caching (saves 70%+ costs)

### **Q: Can I use local models (Ollama)?**

**A:** Yes! FluxGraph works with any provider:
```python
from fluxgraph.models import OllamaProvider

llm = OllamaProvider(model="llama3", base_url="http://localhost:11434")
```

### **Q: How much does it cost to run?**

**A:** FluxGraph itself is free (MIT license). You only pay for:
- LLM API calls (reduced 70%+ with caching)
- Infrastructure (Postgres, Redis â€” pennies/day)

### **Q: Is it production-ready?**

**A:** Yes. FluxGraph powers production systems processing millions of requests. Features include:
- 99.9% uptime (with proper deployment)
- Automatic failover & recovery
- Horizontal scaling
- Enterprise security

### **Q: Can I self-host everything?**

**A:** Absolutely. Use Ollama for LLMs, local Postgres, and you're 100% self-hosted.

### **Q: What about vendor lock-in?**

**A:** Zero lock-in. Switch LLM providers with one line of code. All data stays in your Postgres database.

---

## ğŸŒŸ Success Stories

> **"FluxGraph cut our LLM costs by 73% with semantic caching. Paid for itself in week 1."**  
> â€” Senior Engineer, FinTech Startup

> **"Workflows that survive crashes = no more 3am pages. Our SRE team loves it."**  
> â€” DevOps Lead, Healthcare SaaS

> **"The verifiable audit logs got us HIPAA compliant in weeks, not months."**  
> â€” CTO, Medical AI Company

> **"Finally, an AI framework that doesn't feel like duct tape and hope."**  
> â€” ML Engineer, E-commerce Platform

---

## ğŸ“Š Architecture Deep Dive

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Client Request                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FluxGraph API Layer                     â”‚
â”‚  â€¢ REST Endpoints  â€¢ Streaming (SSE)  â€¢ WebSocket    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Orchestrator (Smart Routing)               â”‚
â”‚  â€¢ Agent Discovery  â€¢ Load Balancing  â€¢ Handoffs     â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚              â”‚              â”‚
      â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent 1 â”‚  â”‚  Agent 2 â”‚  â”‚  Agent N â”‚
â”‚  (GPT-4) â”‚  â”‚ (Claude) â”‚  â”‚ (Gemini) â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Infrastructure Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Postgres â”‚  â”‚  Redis   â”‚  â”‚ ChromaDB â”‚         â”‚
â”‚  â”‚(Workflow)â”‚  â”‚ (Cache)  â”‚  â”‚  (RAG)   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

<div align="center">

## â­ Star Us on GitHub!

**If you're tired of:**
- ğŸ”¥ Rebuilding the same infrastructure for every AI project
- ğŸ’¸ Debugging why your LLM costs are out of control
- ğŸ•µï¸ Flying blind without production logs
- â° Losing hours of work to server crashes

**Then FluxGraph is for you.**

<br>

### **Star us if production AI shouldn't be this hard** â­

<br>

[![GitHub Stars](https://img.shields.io/github/stars/ihtesham-jahangir/fluxgraph?style=social)](https://github.com/ihtesham-jahangir/fluxgraph)

<br>

---

<table>
<tr>
<td align="center" width="33%">
<h3>ğŸš€ Get Started</h3>
<code>pip install fluxgraph[all]</code>
<br><br>
<a href="https://fluxgraph.readthedocs.io/quickstart">Quickstart Guide â†’</a>
</td>
<td align="center" width="33%">
<h3>ğŸ’¬ Join Community</h3>
1000+ developers building with FluxGraph
<br><br>
<a href="https://discord.gg/VZQZdN26">Join Discord â†’</a>
</td>
<td align="center" width="33%">
<h3>ğŸ“– Read Docs</h3>
Comprehensive guides & API reference
<br><br>
<a href="https://fluxgraph.readthedocs.io">View Docs â†’</a>
</td>
</tr>
</table>

---

<sub>**FluxGraph 3.2** â€” Production-Grade AI Agent Framework</sub>  
<sub>Built with ğŸ’™ by developers who've been burned by "toy" frameworks</sub>

<sub>**License:** MIT | **Maintained:** Actively | **Status:** Production-Ready</sub>

[â¬† Back to Top](#fluxgraph)

</div>