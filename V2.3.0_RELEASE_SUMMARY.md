# FluxGraph v2.3.0 Release Summary

## üåê Major Update: Web Scraping & Document Extraction

**Release Date:** January 2, 2026

---

## New Features Overview

### 1. Web Scraping
Extract content from any URL with automatic cleaning and smart parsing:
- Automatic HTML cleaning and normalization
- Smart article extraction with metadata
- Markdown conversion support
- Batch processing with concurrency control
- Headers and authentication support

### 2. Document Extraction
Universal document processing supporting multiple formats:
- **PDF** - Multi-page text extraction with metadata
- **Word** - DOCX and DOC support with table extraction
- **Excel** - XLSX and XLS with sheet and table parsing
- **PowerPoint** - PPTX slide text extraction
- **Structured Data** - JSON, CSV, XML, YAML parsing

### 3. Data Parsing
Automatic extraction of structured data from text:
- Email addresses
- URLs and domains
- Phone numbers (international formats)
- Dates and timestamps
- Prices and currencies
- Bitcoin addresses
- Credit card numbers (with masking)
- Social Security Numbers (with masking)

---

## Convenience Functions

### Web Scraping
```python
from fluxgraph.extractors import scrape_url, scrape_urls, extract_article

# Single URL
content = scrape_url("https://example.com")

# Batch scraping
results = scrape_urls(["url1", "url2", "url3"], max_workers=5)

# Article extraction
article = extract_article("https://blog.example.com/post")
```

### Document Extraction
```python
from fluxgraph.extractors import extract_text, extract_data

# Extract text from any document
text = extract_text("document.pdf")
text = extract_text("report.docx")
text = extract_text("data.xlsx")

# Extract structured data
data = extract_data("contacts.txt", data_types=["emails", "phones"])
```

---

## Installation

### Option 1: Extractors Only
```bash
pip install fluxgraph[extractors]
```
Includes: httpx, beautifulsoup4, pypdf, python-docx, openpyxl, python-pptx, markdownify, lxml

### Option 2: Everything
```bash
pip install fluxgraph[all]
```
Includes all features: extractors, production, security, orchestration, RAG, analytics, and more

---

## New Modules

### 1. `fluxgraph.extractors.web_scraper`
**Classes:**
- `WebScraper` - Main web scraping class with concurrent support

**Functions:**
- `scrape_url()` - Quick single URL scraping
- `scrape_urls()` - Batch scraping with concurrency
- `extract_article()` - Smart article extraction

### 2. `fluxgraph.extractors.document_extractor`
**Classes:**
- `DocumentExtractor` - Universal document processor

**Functions:**
- `extract_text()` - Extract text from any supported format
- `extract_metadata()` - Get document metadata

### 3. `fluxgraph.extractors.data_parser`
**Classes:**
- `DataParser` - Pattern-based data extraction

**Functions:**
- `extract_all_data()` - Extract all data types
- `extract_emails()` - Email extraction
- `extract_urls()` - URL extraction
- `extract_phones()` - Phone number extraction
- `extract_dates()` - Date extraction
- `extract_prices()` - Price/currency extraction

---

## Use Cases

### 1. Content Aggregation
```python
from fluxgraph import Agent
from fluxgraph.extractors import scrape_urls

# Scrape multiple news sources
urls = ["https://news1.com", "https://news2.com", "https://news3.com"]
articles = scrape_urls(urls)

# Use agent to summarize
agent = Agent(
    name="Summarizer",
    instruction="Summarize these news articles"
)
summary = agent.run(context={"articles": articles})
```

### 2. Document Processing Pipeline
```python
from fluxgraph.extractors import extract_text

# Process documents in a directory
import os

documents = []
for file in os.listdir("docs/"):
    if file.endswith(('.pdf', '.docx', '.xlsx')):
        text = extract_text(f"docs/{file}")
        documents.append({"filename": file, "content": text})

# Index in RAG system
from fluxgraph.core import UniversalRAG

rag = UniversalRAG()
for doc in documents:
    rag.add_document(doc["content"], metadata={"source": doc["filename"]})
```

### 3. Lead Generation
```python
from fluxgraph.extractors import scrape_url, extract_all_data

# Scrape company website
content = scrape_url("https://company.com/contact")

# Extract contact information
contacts = extract_all_data(content, data_types=["emails", "phones"])

print(f"Found {len(contacts['emails'])} emails")
print(f"Found {len(contacts['phones'])} phone numbers")
```

### 4. Research Assistant
```python
from fluxgraph import Agent
from fluxgraph.extractors import scrape_urls

agent = Agent(
    name="Researcher",
    instruction="Research the topic and provide a comprehensive summary",
    tools=[scrape_urls]
)

result = agent.run("Research the latest developments in AI agent frameworks")
```

---

## Breaking Changes
None - This is a backward-compatible feature addition.

---

## Dependencies Added
When installing `fluxgraph[extractors]`:
- httpx >= 0.25.0
- beautifulsoup4 >= 4.12.0
- pypdf >= 3.17.0
- python-docx >= 1.0.0
- openpyxl >= 3.1.0
- python-pptx >= 0.6.21
- markdownify >= 0.11.0
- lxml >= 4.9.0

---

## Technical Details

### Web Scraper Features
- Async/concurrent URL fetching
- Automatic retry with exponential backoff
- Custom headers and authentication
- Timeout configuration
- Article metadata extraction (title, author, publish date)
- HTML to Markdown conversion

### Document Extractor Features
- Multi-format support (PDF, DOCX, XLSX, PPTX)
- Table extraction from documents
- Metadata preservation
- Character encoding detection
- Error handling for corrupted files

### Data Parser Features
- Regex-based pattern matching
- Validation and deduplication
- Sensitive data masking (SSN, credit cards)
- International format support (phones, dates)
- Customizable patterns

---

## Performance

- **Web Scraping**: Up to 10 concurrent requests by default
- **Batch Processing**: Configurable worker pool size
- **Memory Efficient**: Streaming support for large documents
- **Error Handling**: Graceful degradation with partial results

---

## Security Considerations

1. **Data Masking**: Automatically masks sensitive data (SSN, credit cards)
2. **Validation**: URL and input validation
3. **Timeouts**: Prevents hanging on slow/dead URLs
4. **Error Handling**: Safe failure modes

---

## Next Steps

1. Upload to PyPI: `twine upload dist/*`
2. Update documentation site
3. Create example notebooks
4. Announce on Discord/Twitter
5. Update GitHub releases

---

## Links

- **PyPI**: https://pypi.org/project/fluxgraph/
- **GitHub**: https://github.com/ihtesham-jahangir/fluxgraph
- **Documentation**: https://fluxgraph.readthedocs.io
- **Discord**: https://discord.gg/Z9bAqjYvPc
- **Twitter**: https://twitter.com/FluxGraphAI

---

**Built with ‚ù§Ô∏è by Ihtesham Jahangir**
